{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation , Dropout\n",
    "import keras.preprocessing.text\n",
    "import keras.backend as K\n",
    "import pickle\n",
    "from keras.metrics import Precision , Recall , Accuracy , TruePositives , TrueNegatives , FalsePositives , FalseNegatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1175358310087892992</td>\n",
       "      <td>لكن بالنهاية  ينتفض  يغير</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1175416117793349632</td>\n",
       "      <td>يعني هذا محسوب على البشر  حيونه ووحشيه  وتطلب...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1175450108898565888</td>\n",
       "      <td>مبين من كلامه خليجي</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1175471073770573824</td>\n",
       "      <td>يسلملي مرورك وروحك الحلوه</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1175496913145217024</td>\n",
       "      <td>وين هل الغيبه  اخ محمد</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1175358310087892992                         لكن بالنهاية  ينتفض  يغير    \n",
       "1  1175416117793349632   يعني هذا محسوب على البشر  حيونه ووحشيه  وتطلب...   \n",
       "2  1175450108898565888                                مبين من كلامه خليجي   \n",
       "3  1175471073770573824                          يسلملي مرورك وروحك الحلوه   \n",
       "4  1175496913145217024                             وين هل الغيبه  اخ محمد   \n",
       "\n",
       "  dialect  \n",
       "0      IQ  \n",
       "1      IQ  \n",
       "2      IQ  \n",
       "3      IQ  \n",
       "4      IQ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final_dialect_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458197, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In order to save time and computation, I will perform stratified sampling and take only 50% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_extra = train_test_split(df, test_size=0.5, stratify=df['dialect'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text will be our training independent x variable and the dialect is our dependent y variable\n",
    "X = df.iloc[:, 1].apply(lambda x: np.str_(x)).values\n",
    "y = df.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text function and remove stop words (Will be passed as a parameter to the count vectorizer)\n",
    "    \n",
    "    Arguments:\n",
    "        text -> Text message which needs to be tokenized\n",
    "    Output:\n",
    "        no_stop_words -> List of tokens from the provided text\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    no_stop_words = [word for word in tokens if word not in stopwords.words('arabic')]\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train-test split (80% training & 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a pipeline that take the features from our text data using count tokens (CountVectorizer) and tf-idf scores (TfidfTransformer) and then it will be passed to a multinomial naive bayes classifier which will give us the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('count', CountVectorizer(tokenizer=tokenize, max_df = 0.8)),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('count',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=0.8,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenize at 0x000001FB0866ED38>,\n",
       "                                 vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          AE       0.68      0.11      0.20      2630\n",
      "          BH       0.68      0.06      0.10      2629\n",
      "          DZ       0.85      0.13      0.22      1618\n",
      "          EG       0.32      0.98      0.48      5764\n",
      "          IQ       0.99      0.06      0.11      1550\n",
      "          JO       0.65      0.05      0.09      2792\n",
      "          KW       0.29      0.76      0.42      4211\n",
      "          LB       0.82      0.38      0.52      2762\n",
      "          LY       0.64      0.52      0.57      3650\n",
      "          MA       0.98      0.14      0.25      1154\n",
      "          OM       0.88      0.02      0.04      1912\n",
      "          PL       0.31      0.64      0.42      4374\n",
      "          QA       0.52      0.33      0.41      3107\n",
      "          SA       0.61      0.11      0.18      2683\n",
      "          SD       0.98      0.04      0.07      1443\n",
      "          SY       1.00      0.02      0.05      1624\n",
      "          TN       1.00      0.01      0.03       924\n",
      "          YE       1.00      0.01      0.01       993\n",
      "\n",
      "    accuracy                           0.37     45820\n",
      "   macro avg       0.73      0.24      0.23     45820\n",
      "weighted avg       0.62      0.37      0.30     45820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = train_test_split(df[['text', 'dialect']], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define hyperparameter\n",
    "# train_args ={\"num_train_epochs\": 4}\n",
    "\n",
    "# # Create a ClassificationModel\n",
    "# model = ClassificationModel(\n",
    "#     \"bert\", \"asafaya/bert-base-arabic\",\n",
    "#     num_labels=18,\n",
    "#     args=train_args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train-test split (80% training & 20% test)\n",
    "X = df['text']\n",
    "y = df['dialect']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max word length\n",
    "max_len = X_train.apply(lambda x: len(str(x).split())).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tokenization & padding sequence\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_seq = token.texts_to_sequences(X_train)\n",
    "X_test_seq = token.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one hot encoding on the target variable (dialect)\n",
    "one_hot = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train_encoded = pd.DataFrame(one_hot.fit_transform(y_train.to_frame()).toarray())\n",
    "y_text_encoded = pd.DataFrame(one_hot.fit_transform(y_test.to_frame()).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns to be their actual names\n",
    "y_train_encoded.rename(columns={0: 'AE', 1: 'BH', 2: 'DZ', 3: 'EG', 4: 'IQ', 5:'JO', 6: 'KW', 7: 'LB', 8: 'LY', 9: 'MA',\n",
    "                                10: 'OM', 11: 'PL', 12: 'QA', 13: 'SA', 14: 'SD', 15: 'SY', 16: 'TN', 17: 'YE'} , inplace=True)\n",
    "y_test.rename(columns={0: 'AE', 1: 'BH', 2: 'DZ', 3: 'EG', 4: 'IQ', 5:'JO', 6: 'KW', 7: 'LB', 8: 'LY', 9: 'MA',\n",
    "                                10: 'OM', 11: 'PL', 12: 'QA', 13: 'SA', 14: 'SD', 15: 'SY', 16: 'TN', 17: 'YE'} , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model \n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_length=max_len))\n",
    "model.add(SimpleRNN(100))\n",
    "model.add(Dense(18, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 87, 300)           109046100 \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                1818      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,088,018\n",
      "Trainable params: 109,088,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5728/5728 [==============================] - 7286s 1s/step - loss: 2.0767 - accuracy: 0.3481\n",
      "Epoch 2/3\n",
      "2157/5728 [==========>...................] - ETA: 1:16:48 - loss: 0.9336 - accuracy: 0.7181"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_pad, y_train_encoded.values, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipeline, open('classifier.pkl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
